GRIMP
===

GRIMP is a data imputation system that imputes missing values in dirty datasets by relying 
on Graph Neural Networks and attention. 

 `main_multilabel.py` is the script to use to run the code.
 
`main_corruption.py` is used to generate dataset that contain missing values according to certain rules.

`run.sh` is a batch script that is used to run experiments on all datasets provided in the script. 

## Installation
We strongly recommend to run the code in a python virtual environment or in a conda environment.

The required packages are listed in `requirements.txt`.

GRIMP also requires pytorch and dgl in order to run. 

To install pytorch in a conda environment, use the command 
```
conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch
```
With pip:
```
pip3 install torch==1.10.0+cu102 torchvision==0.11.1+cu102 torchaudio===0.10.0+cu102 -f https://download.pytorch.org/whl/cu102/torch_stable.html
```

To install DGL in conda:
```
conda install -c dglteam dgl
```
With pip:
```
pip install dgl -f https://data.dgl.ai/wheels/repo.html
```

# Running the code
Example configuration
```
python 
main_multilabel.py
--ground_truth ground/truth/path.csv # Path of the clean file
--dirty_dataset dirty/dataset/path # Path of the dirty file
--gnn_feats 64 # Number of features in the GNN
--h_feats 64 # Number of hidden features in the classifier
--loss xe # cross-entropy loss
--epochs 150 # number of epochs to train the model for 
--grace 400 # number of epochs guaranteed to train for before early stopping can trigger
--dropout_clf 0.2 # dropout in classifier 
--max_components 64 # dimension reduction of pretrained embeddings
--text_emb pretrained/embeddings/path.emb  # path to pretrained embeddings
--head_model attention # using attention in the classifier heads
--shared_model linear # using simple linear modules in the shared layer
--learning_rate 0.001 
--save_imputed_df # the system will output the imputed dataset in results/imputed_datasets
--cat_columns cols_to_convert # list of numerical columns that should be treated as categorical (e.g. IDs, ZIPs)
--fd_path fd/file/path.txt # path to the fd file, if present
```

# Preparing pretrained embeddings
The pretrained embeddings can be generated by using the `prepare_pretrained_embeddings.py` script
in the main folder. 

This script is expecting the fasttext commoncrawl pretrained embedding corpus, 
available at `https://fasttext.cc/docs/en/english-vectors.html`.
It will then list the files in the directory `data/to_pretrain` and generate embeddings for each of them, saving them
in the directory `data/pretrained-emb`. The dataset in `to_pretrain` should be the exact same datasets that will be used
in the training procedure as "dirty datasets".