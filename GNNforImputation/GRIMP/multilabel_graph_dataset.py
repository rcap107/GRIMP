#GiG
import os

import pandas as pd
import numpy as np
import torch
import dgl
from dgl.data import DGLDataset
import random
import copy
import os.path as osp
import pickle
import warnings
from itertools import combinations
from sklearn.preprocessing import minmax_scale


class ImputationTripartiteGraphMultilabelClassifier(DGLDataset):
    def __init__(self, original_data_file_name, missing_value_file_name, random_init=False,
                 node_mapping=None, ext_features=None, pos_neg_scale_factor=20, smart_sampling=False, architecture='multilabel',
                 training_subset='target', target_columns=None, training_columns=None, ignore_columns=None, ignore_num_flag=False,
                 convert_columns=None, norm_strategy='norm', scaling_factor=2,
                 keep_mask=False, unidirectional=True,
                 flag_col=False, flag_rid=False,
                 fd_dict=None, fd_strategy=None, only_rhs=True,
                 max_comb_size=1, max_num_comb=15,
                 training_sample=1, device='cpu'):
        self.graph_name = ''
        self.original_data_file_name = original_data_file_name
        self.missing_value_file_name = missing_value_file_name
        self.distinguish = True
        self.unidirectional = unidirectional

        self.device = device

        # The following values are initialized here and filled later after loading and studying the datasets.
        self.num_rows = self.num_columns = 0
        # num(rows) + num(columns)
        self.num_row_col_nodes = 0 # used as offset for the dataset values
        # num(rows) + num(columns) + num(distinct cell values in original_data_file_name)
        self.num_total_nodes = 0
        self.input_tuple_length = 0
        self.missing_values_by_col = dict()
        self.scaling_factor = scaling_factor

        # This parameter defines whether the train/valid sets should be kept or if they should be generated randomly
        # during each execution
        self.keep_mask=keep_mask

        # Tuple-building parameters
        self.flag_index = -1
        self.flag_value = '__flag_index__'
        # if flag_col or flag_rid:
        #     raise NotImplementedError(f'These flags don\'t work with the attention layer for the moment. ')
        self.flag_col = flag_col
        self.flag_rid = flag_rid

        # Parameters for size of training set
        assert 0.0 < training_sample <= 1.0
        self.training_sample = training_sample

        # To handle rows with multiple missing values, sample sets can have a variable length. This list keeps track
        # of the indices of the first/last value of each set of samples.
        self.boundaries_train = []
        self.boundaries_valid = []

        # Parameters for negative sampling
        # Scale factor: number of negative training values for each positive training value.
        self.positive_negative_train_scale_factor = pos_neg_scale_factor
        # Boolean, whether to do smart sampling or not.
        self.smart_sampling = smart_sampling

        # Additional (optional) data structure that saves labels so they're not computed online
        self.labels = None

        # Additional parameter to specify random initialization of all features
        # Initial features are set to 1.0 if random_init is False.
        self.random_init = True

        # Additional data structures needed to handle external features.
        # These are generated by an external function
        self.node_mapping = node_mapping
        self.ext_features = ext_features

        # Parameters for handling FDs
        self.fd_dict = fd_dict
        if fd_dict is None:
            self.fd_strategy = None
        else:
            self.fd_strategy = fd_strategy
        self.flat_fds = dict()
        self.only_rhs = only_rhs
        self.num_fds = 0

        # Model architecture (either multitask or multilabel)
        self.architecture = architecture
        # Subset of columns to be used when performing training. Either all columns, missing columns or columns defined
        # in variable "training_columns".
        self.training_subset = training_subset
        self.target_columns = target_columns
        self.training_columns = training_columns
        self.ignore_columns = ignore_columns
        self.ignore_num_flag = ignore_num_flag
        self.convert_columns = convert_columns
        self.norm_strategy = norm_strategy

        self.dirty_columns = []

        # Maximum size of combinations to be used in the generation of additional missing values
        # WARNING: large numbers of dirty columns and/or large values of max_comb_size will cause the system to run out of memory!!!
        self.max_comb_size = max_comb_size
        self.max_num_comb = max_num_comb

        super().__init__(name='EmbDI Graph')
        self.graph_name = 'multilabel'

    def _prepare_train_test_columns(self):
        # If target_columns is None, then all columns that contain missing values are assumed to be imputation targets.
        if self.target_columns is None:
            self.target_columns = self.df_missing.columns[self.df_missing.isna().any()].to_list()

        if self.ignore_num_flag:
            self.ignore_columns = self.numerical_columns.to_list()

        if self.ignore_columns is not None:
            self.target_columns = [target_column for target_column in self.target_columns if target_column not in self.ignore_columns]

        # If self.training_columns is not None, the training columns have already been supplied.
        if self.training_columns is None:
            if self.training_subset == 'target':
                # Train only on the target columns.
                self.training_columns = self.target_columns
            elif self.training_subset == 'missing':
                # Train on all columns that contain missing values (this subset might be different from the target subset).
                self.training_columns = self.df_missing.columns[self.df_missing.isna().any()].to_list()
                # Train on all columns in the dataset.
            elif self.training_subset == 'all':
                if self.ignore_columns is not None:
                    self.training_columns = [col for col in self.df_missing.columns.to_list() if col not in self.ignore_columns]
                else:
                    self.training_columns = self.df_missing.columns.to_list()
            else:
                raise ValueError(f'Unknown traing subset {self.training_subset}')

        # if self.target_columns is not None:
        # Check to ensure that all target columns are present among the training columns.
        for col in self.target_columns:
            if col not in self.training_columns:
                raise ValueError(f'Designated target column "f{col}" was not found among training columns. ')
        self.training_columns.sort(key=self.df_orig.columns.to_list().index)

    def _prepare_node_dictionaries(self):
        # These dictionaries are used to map values between the original dataframes (with strings)
        # and the data in the model (node indices)

        # Map values to their indices, and viceversa
        self.val2idx = dict((v, i + self.num_row_col_nodes) for (i, v) in enumerate(sorted(self.distinct_value_set)))
        self.idx2val = dict((i + self.num_row_col_nodes, v) for (i, v) in enumerate(sorted(self.distinct_value_set)))

        # Map columns to their indices, and viceversa
        self.col2idx = dict((col, self.num_rows + i) for (i, col) in enumerate(self.df_missing.columns))
        self.idx2col = dict((self.num_rows + i, col) for (i, col) in enumerate(self.df_missing.columns))

        self.node2idx = {f'idx__{idx}':idx for idx in range(self.num_rows)}
        self.node2idx.update({f'cid__{cid}':v for cid, v in self.col2idx.items()})
        self.node2idx.update({f'tt__{token}':v for token, v in self.val2idx.items()})
        self.idx2node = {v:k for k, v in self.node2idx.items()}

        #For each distinct value associate with it the column in which it is found
        #if a value V is present in two columns A and B, then it will be associated with the latest column
        self.cell_to_dict = {self.df_missing.iloc[row, col]: col for row in range(self.num_rows) for col in range(self.num_columns)}

    def _measure_value_counts(self):
        # Measure the number of occurrences of each unique value in the dataset
        # This is useful to have an idea of the degree distribution in the graph.

        self.counts = {k: None for k in self.df_missing.columns}

        self.frequencies = {k: dict() for k in self.df_missing.columns}
        self.frequencies_scaled = {k: dict() for k in self.df_missing.columns}

        for column in self.counts:
            self.counts[column] = self.df_missing.value_counts(column)
            tmp_df = (self.counts[column] / len(self.df_missing)).reset_index()
            tmp_df.columns = [column, 'frequency']
            tmp_df[column] = tmp_df[column].apply(lambda x: self.val2idx[x])
            self.frequencies[column].update({int(row[0]) : row[1] for row in tmp_df.values})
            lower_bound = max(tmp_df['frequency'])
            upper_bound = self.scaling_factor*lower_bound
            s = minmax_scale(list(self.frequencies[column].values()), feature_range=(lower_bound, upper_bound))/upper_bound
            self.frequencies_scaled[column] = dict(zip(self.frequencies[column].keys(), s))

        # self.frequencies[column] = tmp_df

            if self.smart_sampling:
                # Clamping most frequent value to perform smart sampling of negative samples
                self.counts[column][0] = self.counts[column][1]

        if self.smart_sampling:
            self.budget = copy.deepcopy(self.counts)
            for column in self.budget:
                self.budget[column][0] = self.budget[column][1]
                self.budget[column] *= self.positive_negative_train_scale_factor
                self.budget[column] = self.budget[column].reset_index(name='counts')

    def load_external_features(self):
        # Get the number of dimensions in the external features
        self.num_features = self.ext_features.shape[1]
        # The size of the of "features" tensor is equal to the number of nodes in the graph.
        # NB: this includes the flag node at the end: therefore, num_total_nodes = ext_features.shape[0]+1
        self.graph.ndata['features'] = torch.zeros(size=(self.num_total_nodes, self.num_features))
        temp_features = torch.zeros(size=(self.num_total_nodes, self.ext_features.shape[1]))
        # If there is a node mapping, node features must be reordered.
        if self.node_mapping is not None:
            # perm is filtered: only the nodes in the mapping that are among graph nodes are selected.
            perm = [self.node_mapping.index(node) for node in self.node2idx if node in self.node_mapping]
            _t = [node for node in self.node_mapping if node not in self.node2idx]
            temp_features = torch.zeros(size=(self.num_total_nodes, self.ext_features.shape[1]))
            temp_features[:len(perm), :] = self.ext_features[perm,:]
        else:
            temp_features[:self.ext_features.shape[0], :] = self.ext_features[:]
        self.graph.nodes[:].data['features'] = temp_features

    def _remove_target_edges(self, target_triplets):
        '''
        This function removes all the edges involved in target_triplets to make sure that the training algorithm is not
        aware of the validation and testing edges.
        :return:
        '''
        u = []
        v = []
        for triplet in target_triplets:
            row_id, col_id, val_id = triplet
            u.append(row_id)
            v.append(val_id)
            if not self.unidirectional:
                u.append(val_id)
                v.append(row_id)
                #
                u.append(val_id)
                v.append(col_id)
            u.append(col_id)
            v.append(val_id)

        u = torch.tensor(u).to(int)
        v = torch.tensor(v).to(int)
        target_edges = self.graph.edge_ids(u,v)
        self.graph.remove_edges(target_edges)

    def prepare_valid_mask(self, valid_fraction=0.2):
        path, ext = osp.splitext(self.missing_value_file_name)
        if self.distinguish:
            # Split nodes
            mask_filename = path + '_mask_d.pkl'
        else:
            # Merged nodes
            mask_filename = path + '_mask_m.pkl'
        n_rows = len(self.df_train)

        if self.keep_mask and osp.exists(mask_filename):
            mask_dict = pickle.load(open(mask_filename, 'rb'))
            try:
                _ = self.df_train.loc[mask_dict['train_idx']]
                _ = self.df_train.loc[mask_dict['valid_idx']]
                if len(_) == 0:
                    raise ValueError(f'Something went wrong while importing the mask. ')
                return mask_dict['train_idx'], mask_dict['valid_idx']
            except KeyError:
                warnings.warn(f'The saved Train/Valid mask is not appropriate for the current combination '
                               f'of training columns.\nA new mask will be generated.')
        else:
            # np.random.choice(self.df_train, int(len(self.df_train) * valid_fraction))
            indexes = np.random.choice(np.arange(n_rows), int(n_rows*valid_fraction))
            mask = np.ones(n_rows, dtype=bool)
            mask[indexes] = False
            train_idx = self.df_train.loc[mask].index
            valid_idx = self.df_train.loc[~mask].index
            mask_dict = {'train_idx': train_idx, 'valid_idx': valid_idx}
            if self.keep_mask:
                pickle.dump(mask_dict, open(mask_filename, 'wb'))
            return train_idx, valid_idx

    def build_validation_df(self):
        # return self.df_train.loc[self.train_idx], \
        #        self.df_train.loc[self.valid_idx]
        return self.df_train.reindex(self.train_idx), \
               self.df_train.reindex(self.valid_idx)

    def _align_floats(self, source_df: pd.DataFrame):
        '''
        This function iterates over the columns and tries to convert them to floats and then back to strings.
        This ensures consistency between datasets when null values are injected in integer columns.
        :return:
        '''

        for col in source_df.columns:
            # if source_df[col].dtype ==
            if col in self.numerical_columns:
            # if source_df[col].str.isnumeric().all():
                source_df[col] = source_df[col].astype(float).astype('str')
            else:
                source_df[col] = source_df[col].astype('str')
        return source_df

    def _distinguish_homographs(self, df):
        for i, col in enumerate(df.columns):
            df[col] = df[col].apply(lambda x: f'c{i}_{x}' if x==x and x!='nan' else np.nan)
        return df

    def _normalize_numeric_columns(self):
        self.normalized_mapping = {col: {} for col in self.numerical_columns}
        if self.norm_strategy == 'minmax':
            self.norm_max = dict()
            self.norm_min = dict()
            for col in self.numerical_columns:
                mmax = self.df_missing[col].max()
                mmin = self.df_missing[col].min()
                self.norm_max[col] = mmax
                self.norm_min[col] = mmin
                for x in self.df_missing[col].unique():
                    self.normalized_mapping[col][x] = (x-mmin)/(mmax-min)
                # self.df_missing[col] = self.df_missing[col].apply(lambda x: )
                # self.df_orig[col] = self.df_orig[col].apply(lambda x: (x-mean)/std)
        elif self.norm_strategy == 'norm':
            self.norm_mean = dict()
            self.norm_std = dict()
            for col in self.numerical_columns:
                mean = self.df_missing[col].mean()
                std = self.df_missing[col].std()
                self.norm_mean[col] = mean
                self.norm_std[col] = std
                for x in self.df_missing[col].unique():
                    self.normalized_mapping[col][x] = (x-mean)/std

                # self.df_missing[col] = self.df_missing[col].apply(lambda x: (x-mean)/std)
                # self.df_orig[col] = self.df_orig[col].apply(lambda x: (x-mean)/std)
        else:
            raise ValueError(f'Unknown normalization strategy {self.norm_strategy}')


    def denormalize_column(self, column, column_values):
        if self.norm_strategy == 'norm':
            return column_values*self.norm_std[column]+self.norm_mean[column]
        if self.norm_strategy == 'minmax':
            return (column_values + self.norm_min[column]) * (self.norm_max[column] - self.norm_min[column])

    def load_and_compute_stats(self):
        self.df_orig = pd.read_csv(self.original_data_file_name)
        self.df_missing = pd.read_csv(self.missing_value_file_name)
        self.all_columns = list(self.df_missing.columns)
        self.numerical_columns = self.df_missing.select_dtypes(include='number').columns
        self.categorical_columns = self.df_missing.select_dtypes(exclude='number').columns
        if self.convert_columns is not None:
            for col in self.convert_columns:
                self.numerical_columns.remove(col)
                self.categorical_columns.append(col)
        self._normalize_numeric_columns()
        self.df_orig  = self._align_floats(self.df_orig)
        self.df_missing = self._align_floats(self.df_missing)
        if self.distinguish:
            self.df_orig = self._distinguish_homographs(self.df_orig)
            self.df_missing = self._distinguish_homographs(self.df_missing)
        self.dirty_columns = self.df_missing.columns[self.df_missing.isna().any()].to_list()
        self._prepare_train_test_columns()

        if self.df_orig.shape != self.df_missing.shape:
            raise Exception("Error: Input files do not have same number of rows and columns")

        self.df_train = self.df_missing.dropna(subset=self.training_columns, axis=0, how='all').copy()
        self.train_idx, self.valid_idx = self.prepare_valid_mask()
        self.df_train, self.df_valid = self.build_validation_df()

        #Convert every attribute to string and treat it as categorical
        #Redundant given the dtype in read_csv but still
        for col in self.df_missing.columns:
            self.df_missing[col] = self.df_missing[col].astype(str)
        for col in self.df_missing.columns:
            self.df_missing[col] = self.df_missing[col].astype(str)

        # Using dtype in read_csv makes missing value as a string 'nan'. So replace it appropriately to nan
        self.df_missing.replace('nan', np.nan, inplace=True)

        # Node id semantics: let n, m, l be number of rows, columns and distinct cell values
        # then node ids 0 to n-1 correspond to rows,
        # node ids n to n+m-1 correspond to columns
        # and n+m to n+m+l-1 correspond to cells
        self.num_rows, self.num_columns = self.df_missing.shape
        self.num_row_col_nodes = self.num_rows + self.num_columns

        self.distinct_value_set = set(self.df_missing.values.ravel().tolist())
        try:
            self.distinct_value_set.remove(np.nan)
        except KeyError:
            raise KeyError('No null values found in the dataset. Are you sure the correct dirty dataset has been supplied?')
        self._prepare_node_dictionaries()

        # Adding one dummy node for indicization later on (needed for array operations)
        self.num_total_nodes = self.num_row_col_nodes + len(self.val2idx) + 1
        self.flag_index = self.num_total_nodes - 1
        self.val2idx[self.flag_value] = self.flag_index
        self.val2idx[np.nan] = self.flag_index
        self.idx2val[self.flag_index] = self.flag_value

        #Aggregate for each column and then aggregate for entire data frame
        self.num_missing_values = 0 # will be computed later. self.df_missing.isna().sum().sum()
        self.num_non_missing_values = 0 # will be created later len(self.distinct_value_set)

        # Prepare dict mappings between node name and vector idx
        self._measure_value_counts()

        # create a dictionary to avoid repeated cell id lookups for each attribute.
        # The key is col_id and it will return cell_ids of all the values in its domain
        self.attribute_domain_cell_id_dict = {}
        self.has_missing = []

        for col in self.df_missing.columns:
            n_missing = sum(self.df_missing[col].isna())
            if n_missing > 0:
                self.has_missing.append(self.col2idx[col])
                self.missing_values_by_col[self.col2idx[col]] = n_missing
            distinct_values = self.df_missing[col].unique()
            cell_id_array = [self.val2idx[val] for val in distinct_values if val == val]
            col_id = self.col2idx[col]
            self.attribute_domain_cell_id_dict[col_id] = cell_id_array

        self.head_dims = {col_id: None for col_id in self.attribute_domain_cell_id_dict}
        for col in self.target_columns:
            col_id = self.col2idx[col]
            if col in self.numerical_columns:
                self.head_dims[col_id] = 1
            else:
                self.head_dims[col_id] = len(self.attribute_domain_cell_id_dict[col_id])

    def compute_number_edges(self):
        # Note that we use df_missing for creating the edges
        # So cells corresponding to missing values will be isolated
        self.num_non_missing_values = (~self.df_missing.isna()).sum().sum()
        self.num_missing_values = (self.df_missing.isna()).sum().sum()

        # num_edges = 1 * self.num_non_missing_values
        num_edges = 2 * self.num_non_missing_values
        return num_edges

    def add_fd_edges_val2val(self):
        '''
        This function adds edges between cell values that are in a FD relation. FDs are loaded in self.fd_dict.
        :return: List of (unique) edges.
        '''
        assert self.fd_dict is not None

        i2c = lambda x: self.df_missing.columns[x]

        new_edges = []
        for idx, row in self.df_missing.iterrows():
            for rhs in self.fd_dict:
                rhs_val = row[i2c(rhs)]
                rhs_idx = self.val2idx[rhs_val]
                fds_subset = self.fd_dict[rhs]
                for fd in fds_subset:
                    self.num_fds+=1
                    lhs_vals = [row[i2c(lhs)] for lhs in fd]
                    lhs_idxs = [self.val2idx[lhs] for lhs in lhs_vals]
                    # unidirectional edge lhs -> rhs
                    new_edges += [(lhs_idx, rhs_idx) for lhs_idx in lhs_idxs]
                    if not self.only_rhs:
                        # bidirectional edges between values within the lhs
                        new_edges += [(l1, l2) for l1 in lhs_idxs for l2 in lhs_idxs if l1!=l2]

        # Remove duplicated edges
        return list(set(new_edges))

    def add_fd_edges_col2col(self):
        '''
        This function adds edges between columns that are in a FD relation. FDs are already loaded in self.fd_dict.
        :return: List of (unique) edges.
        '''
        assert self.fd_dict is not None

        i2c = lambda x: self.df_missing.columns[x]

        new_edges = []
        for rhs in self.fd_dict:
            rhs_val = i2c(rhs)
            rhs_idx = self.col2idx[rhs_val]
            fds_subset = self.fd_dict[rhs]
            for fd in fds_subset:
                self.num_fds+=1
                lhs_idxs = [self.col2idx[i2c(lhs)] for lhs in fd]
                # unidirectional edge lhs -> rhs
                new_edges += [(lhs_idx, rhs_idx) for lhs_idx in lhs_idxs]

        # Remove duplicated edges
        return list(set(new_edges))

    def add_inrow_edges(self):
        new_edges = []
        for idx, row in self.df_missing.iterrows():
            tmp_edges = [(l1, l2) for l1 in row for l2 in row if l1!=l2 and (l1 is not np.nan and l2 is not np.nan) ]
            new_edges += [(self.val2idx[l1], self.val2idx[l2]) for l1,l2 in tmp_edges]
        return list(set(new_edges))

    def create_graph(self):
        #Create empty graph
        self.graph = dgl.graph(data=[])

        #Create num_total_nodes isolated nodes
        self.graph.add_nodes(self.num_total_nodes)

        #Create edges
        num_edges = self.compute_number_edges()

        #DGL expects the edges to be specified as two tensors containing start and end nodes.
        start_nodes_l = []
        end_nodes_l   = []
        edge_weights = []
        # start_nodes = torch.zeros(num_edges, dtype=torch.int64)
        # end_nodes   = torch.zeros(num_edges, dtype=torch.int64)

        index = 0

        for row in range(self.num_rows):
            row_node_id = row
            for col in range(self.num_columns):
                col_node_id = col + self.num_rows
                if pd.isnull(self.df_missing.iloc[row, col]) == False:
                    cell_node_id = self.val2idx[self.df_missing.iloc[row, col]]
                    start_nodes_l.append(row_node_id)
                    end_nodes_l.append(cell_node_id)
                    start_nodes_l.append(col_node_id)
                    end_nodes_l.append(cell_node_id)
                    edge_weights.append(1.0)
                    edge_weights.append(1.0)
                    # start_nodes[index] = row_node_id
                    # end_nodes[index] = cell_node_id
                    # start_nodes[index+1] = col_node_id
                    # end_nodes[index+1] = cell_node_id
                    index = index + 2
                # else:
                #     for val in self.attribute_domain_cell_id_dict[col_node_id]:
                #         cell_node_id = val
                #         start_nodes_l.append(row_node_id)
                #         end_nodes_l.append(cell_node_id)
                #         edge_weights.append(self.frequencies_scaled[self.all_columns[col]][cell_node_id]/2)
                #         index = index + 1

        start_nodes = torch.tensor(start_nodes_l, dtype=torch.int64)
        end_nodes = torch.tensor(end_nodes_l, dtype=torch.int64)

        weights_tensor = torch.tensor(edge_weights, dtype=torch.float32)


        #Make the graph undirected
        self.graph.add_edges(start_nodes, end_nodes, data={'features':weights_tensor})
        if not self.unidirectional:
            self.graph = dgl.add_reverse_edges(self.graph, copy_edata=True)
        # If needed, prepare FD edges and add the count to the edges tensor
        if False : #and self.fd_dict is not None:
            if self.fd_strategy == 'val2val':
                fd_edges = self.add_fd_edges_val2val()
            elif self.fd_strategy == 'col2col':
                fd_edges = self.add_fd_edges_col2col()
            elif self.fd_strategy == 'v2vc2c':
                fd_edges = []
                fd_edges += self.add_fd_edges_val2val()
                fd_edges += self.add_fd_edges_col2col()
            elif self.fd_strategy is None:
                raise ValueError(f'No FD strategy provided. Aborting.')
            else:
                raise ValueError(f'Unknown FD strategy {self.fd_strategy}')

            print(f'Added {len(fd_edges)} FD-based edges .')
            num_edges = len(fd_edges)
            start_nodes = torch.zeros(num_edges, dtype=torch.int64)
            end_nodes   = torch.zeros(num_edges, dtype=torch.int64)

            for e_num, edge in enumerate(fd_edges):
                start_nodes[e_num] = edge[0]
                end_nodes[e_num] = edge[1]
                # index += 1
            self.graph.add_edges(start_nodes, end_nodes)
            self.graph = dgl.add_reverse_edges(self.graph)

        if self.fd_dict is not None:
            for col in self.fd_dict:
                self.flat_fds[col] = self.get_flat_fds(col)

        self.graph = self.graph.to_simple(copy_edata=True)
        if not self.unidirectional:
            self.graph = dgl.add_self_loop(self.graph)
        self.graph.edata['features'][self.graph.edata['features'] == 0]=1.0
        # self.graph = self.graph.to_simple()

        print("Graph has %d nodes and %d edges"%(self.graph.number_of_nodes(), self.graph.number_of_edges()))

    def compute_node_features(self):
        #ndata stores various features for each node.
        scaling = 1
        self.num_features = self.num_columns * scaling
        self.graph.ndata['features'] = torch.zeros( (self.num_total_nodes, self.num_features) )

        #Features for row nodes: all rows are connected to all columns
        self.graph.nodes[range(0, self.num_rows)].data['features'] = torch.ones(self.num_rows, self.num_features)

        #Features for column nodes: columns are connected only to themselves
        self.graph.nodes[range(self.num_rows, self.num_rows+self.num_columns)].data['features'] = torch.eye(self.num_columns).repeat_interleave(scaling,1)

        #Features for cell nodes: each node is connected to each column
        self.graph.nodes[range(self.num_row_col_nodes, self.flag_index)].data['features'] = torch.ones(self.num_total_nodes-(self.num_row_col_nodes+1), self.num_features)

        if self.random_init:
            # Multiply all values by a random amount, to differentiate better between nodes.
            self.graph.nodes[range(0, self.num_total_nodes)].data['features'] *= torch.FloatTensor(self.num_total_nodes, self.num_features).uniform_()

    def create_train_positive_graph(self):
        self.input_tuple_length = len(self.df_missing.columns)
        # if self.fd_dict is not None:
        #     self.input_tuple_length *= 2
        if self.flag_rid:
            # RID vector will be added at the beginning of the tuple.
            self.input_tuple_length += 1
        if self.training_sample < 1.0:
            train_df = self.df_train.sample(frac=self.training_sample)
        else:
            train_df = self.df_train.copy()

        # # Samples contain the indices of the nodes in each tuple
        # self.train_positive_samples = np.zeros((self.train_positive_samples_size, self.input_tuple_length))
        # # Triplets contain the row and column ids of the target value, as well as the correct filling.
        # self.train_positive_triplets_size = len(self.df_train) * len(self.training_columns)
        # self.train_positive_triplets = np.zeros((self.train_positive_triplets_size, 3))
        # self.get_positive_negative_samples_for_training()
        self.train_positive_triplets, self.train_positive_samples, self.train_positive_samples_size, self.boundaries_train = self.get_positive_samples(train_df)
        print("Size of training positive samples ", self.train_positive_samples_size)

    def create_valid_positive_graph(self):
        # # The number of samples and triplets is given by the size of the df_train dataset (one positive sample per cell)
        # self.valid_positive_samples_size = len(self.df_valid)*len(self.training_columns)
        #
        # # Samples contain the indices of the nodes in each tuple
        # self.valid_positive_samples = np.zeros((self.valid_positive_samples_size, self.input_tuple_length))
        #
        # # Triplets contain the row and column ids of the target value, as well as a possible filling (either correct or wrong)
        # self.valid_positive_triplets_size = len(self.df_valid)*len(self.training_columns)
        #
        # self.valid_positive_triplets = np.zeros((self.valid_positive_triplets_size, 3))
        #
        # self.get_positive_negative_samples_for_validation()
        self.valid_positive_triplets, self.valid_positive_samples, self.valid_positive_samples_size, self.boundaries_valid = self.get_positive_samples(self.df_valid)
        print("Size of validation positive samples ", self.valid_positive_samples_size)
        self._remove_target_edges(self.valid_positive_triplets)

    def create_test_positive_negative_graphs(self):
        # self.test_positive_samples = torch.zeros(size=(self.test_positive_samples_size,1))
        # # self.test_negative_samples = torch.zeros(size=(self.test_positive_samples_size,1))
        # self.test_positive_samples_size = self.num_missing_values
        # self.test_negative_samples_size = self.num_missing_values*self.positive_negative_train_scale_factor
        #
        # # self.test_positive_samples = [0 for _ in range(self.test_positive_samples_size)]
        # # self.test_negative_samples = [0 for _ in range(self.test_negative_samples_size)]
        # self.test_positive_samples = []
        # self.get_positive_negative_samples_for_testing()
        #
        self.test_positive_triplets, self.test_positive_samples, self.test_positive_samples_size, self.boundaries_test = self.get_samples_for_testing()

        print("Number of testing samples: ", self.test_positive_samples_size)

    def generate_labels(self, triplets):
        labels = torch.zeros(triplets.shape[0], dtype=int)
        weights = torch.zeros(triplets.shape[0], dtype=float)

        for idx,sample in enumerate(triplets):
            row_id, col_id, val_id = tuple(map(int, sample))
            if val_id == self.flag_index:
                continue
            else:
                labels[idx] = int(val_id) - self.num_row_col_nodes
                weights[idx] = self.frequencies[self.idx2col[col_id]][val_id]
                self.size_target_col = len(self.attribute_domain_cell_id_dict[col_id])
        return labels, weights

    def generate_labels_multitask(self, triplets):
        labels = {col_id: [] for col_id, val in self.attribute_domain_cell_id_dict.items()}
        weights = {col_id: [] for col_id, val in self.attribute_domain_cell_id_dict.items()}

        for idx, sample in enumerate(triplets):
            row_id, col_id, val_id = tuple(map(int, sample))
            col = self.idx2col[col_id]
            if val_id == self.flag_index:
                continue
            else:
                if col in self.categorical_columns:
                    labels[col_id].append(self.attribute_domain_cell_id_dict[col_id].index(val_id))
                    weights[col_id].append(self.frequencies[self.idx2col[col_id]][val_id])
                elif col in self.numerical_columns:
                    val = self.idx2val[val_id]
                    _, real_value = val.split('_')
                    norm_value = self.normalized_mapping[col][float(real_value)]
                    labels[col_id].append(float(norm_value))
                else:
                    raise ValueError(f'Something is wrong with column {col}')
        labels = {col: v for col, v in labels.items() if v is not None}
        labels_t = []
        for col_id in labels:
            col = self.idx2col[col_id]
            if len(labels[col_id])==0:
                labels_t.append([])
            if col in self.categorical_columns:
                labels_t.append(torch.tensor(labels[col_id], dtype=torch.long, device=self.device))
            else:
                labels_t.append(torch.tensor(labels[col_id], dtype=torch.float32, device=self.device))
        # labels_t = [torch.LongTensor(labels[col]) if self.head_dims[col] > 1 else torch.FloatTensor(labels[col]) for col in labels]
        weights_t = [torch.FloatTensor(weights[col]) for col in weights if len(weights[col]) > 0]
        return labels_t , weights_t

    def check_triplet(self, triplet):
        row_id, col_id, val_id = triplet
        if val_id not in self.graph.in_edges(row_id)[0]:
            return False
        # if val_id not in self.graph.in_edges(col_id)[0]:
        #     return False
        # if col_id not in self.graph.in_edges(val_id)[0] or row_id not in self.graph.in_edges(val_id)[0] :
        #     return False

        return True

    @staticmethod
    def get_combinations(target_columns, max_comb_size, max_num_comb, min_comb_size=1):
        from operator import itemgetter
        if max_comb_size == 0:
            max_comb_size = len(target_columns)-1
        if max_comb_size < 1:
            raise ValueError(f'Combinations must have at least size 1. Current size: {max_comb_size}')
        if max_comb_size == 1:
            print(f'Max combination size == 1: no additional training samples. ')
            return [[_] for _ in target_columns]
        if max_comb_size >= len(target_columns):
            raise ValueError(f'Max combination size must be smaller than the number of target columns. '
                             f'\nmax_comb_size={max_comb_size} - len(target_columns)={len(target_columns)}')
        if max_comb_size == len(target_columns) - 1:
            max_num_comb = len(target_columns)
        comb_dict = {l: [] for l in range(max_comb_size, min_comb_size-1, -1)}
        total_combs = 0
        for comb_len in range(max_comb_size, min_comb_size-1, -1):
            for comb in combinations(target_columns, comb_len):
                comb_dict[comb_len].append(comb)
                total_combs+=1
            random.shuffle(comb_dict[comb_len])
        n_chosen_combs = min(max(max_num_comb, len(target_columns)), total_combs)

        chosen_combs = []
        counter = 0
        def pick_comb(col):
            for idx, c in enumerate(cl):
                if col in c:
                    yield cl.pop(idx)

        cl = comb_dict[max_comb_size]
        if n_chosen_combs == len(cl):
            comb_list = cl
        else:
            comb_list = []
            counter = 0
            while len(comb_list) < n_chosen_combs:
                col = target_columns[counter]
                tgtc = pick_comb(col).__next__()
                comb_list.append(tgtc)
                counter = (counter+1)%len(target_columns)

        print(f'Generation of combinations of training columns.')
        print(f'Keeping {n_chosen_combs}/{total_combs} combinations.')
        return comb_list

    def get_flat_fds(self, col):
        fds = self.fd_dict[col]
        flat_fds = [l for fd in fds for l in fd]
        return flat_fds

    def get_fd_context(self, v, col):
        if col in self.fd_dict:
            flat_fds = self.get_flat_fds(col)
            fd_tuple = [val  if idx in flat_fds else self.flag_index for idx, val in enumerate(v)]
        else:
            fd_tuple = [self.flag_index for idx, val in enumerate(v)]
        return v + fd_tuple

    def get_positive_samples(self, source_df):
        '''
        This function prepares the training/validation samples based on the content of the source_df.
        :param source_df:
        :return:
        '''
        positive_samples  = 0
        positive_triplets = 0
        train_positive_samples  = []
        train_positive_triplets = []
        boundaries = []

        problems_by_col = {col: 0 for col in source_df.columns}

        c_idx = {v: k for k,v in enumerate(self.df_orig.columns)}
        comb_list = self.get_combinations(self.training_columns, self.max_comb_size, self.max_num_comb)
        self.num_combs = len(comb_list)

        for iteration, col in enumerate(self.training_columns):
            boundaries.append(positive_triplets)
            col_num = source_df.columns.to_list().index(col)
            col_id = self.col2idx[col]
            for row_num, row in source_df.iterrows():
                val = source_df.loc[row_num, col]
                val_id = self.val2idx[val]
                # Check if the current value has the same index as the null value, if so, skip this triplet.
                if val_id == self.flag_index:
                    continue
                full_tuple = source_df.loc[row_num].tolist()
                triplet = (row_num, col_id, val_id)
                if not self.check_triplet(triplet):
                    # pass
                    raise ValueError
                for comb in comb_list:
                    if col in comb:
                        v = [self.val2idx[v] for v in full_tuple]
                        for comb_c in comb:
                            if self.flag_col:
                                # Adding column vector to fill in for missing value.
                                v[c_idx[comb_c]] = self.col2idx[comb_c]
                            else:
                                # Adding "dummy" vector position for later reindexing
                                v[c_idx[comb_c]] = self.flag_index
                        if self.flag_rid:
                            # Adding rid to the beginning of the vector.
                            v = [row_num] + v
                        # if self.fd_dict is not None:
                        #     v = self.get_fd_context(v, c_idx[col])
                        train_positive_samples.append(v)
                        train_positive_triplets.append(triplet)
                        positive_triplets += 1
                        positive_samples += 1
        # boundaries.append(positive_triplets)

        og_triplets_tensor = torch.tensor(train_positive_triplets, dtype=int)
        og_samples_tensor = torch.tensor(train_positive_samples, dtype=int)

        print(og_samples_tensor.shape)

        uniq_concat = torch.unique(torch.cat([og_samples_tensor, og_triplets_tensor], dim=1), dim=0)
        sorted_uc = uniq_concat[np.argsort(uniq_concat[:, -2], axis=0)]
        uniques, boundaries = np.unique(sorted_uc[:, -2], return_index=True)
        boundaries = np.concatenate([boundaries, [len(sorted_uc)]])
        samples_tensor = sorted_uc[:, :-3]
        print(samples_tensor.shape)
        samples_size = len(samples_tensor)
        triplets_tensor = sorted_uc[:, -3:]
        return triplets_tensor, samples_tensor, samples_size, boundaries

    def get_samples_for_testing(self):
        positive_samples  = 0
        positive_triplets = 0
        test_samples  = []
        test_triplets = []
        boundaries = []
        self.impossible = []
        problems_by_col = {col: 0 for col in self.df_missing.columns}

        for iteration, col in enumerate(self.training_columns):
        # for iteration, col in enumerate(self.target_columns):
            boundaries.append(positive_triplets)
            if col not in self.target_columns:
                continue
            col_num = self.df_missing.columns.to_list().index(col)
            col_id = self.col2idx[col]
            for row_num, row in self.df_missing.iterrows():
                # self.train_positive_samples[positive_samples, 0] = row_num
                # col = self.idx2col[target_column]
                val = self.df_missing.loc[row_num, col]
                if self.df_missing.columns[col_num] in self.target_columns and \
                        pd.isnull(self.df_missing.iloc[row_num, col_num]) == True:
                    true_value = self.df_orig.iloc[row_num, col_num]
                    if col in self.numerical_columns:
                        prefix, val = true_value.split('_')
                        true_value_id = float(val)
                    else:
                        try:
                            true_value_id = self.val2idx[true_value]
                        except KeyError:
                            # print(f'{true_value} cannot be imputed.')
                            self.impossible.append(true_value)
                            continue

                    full_tuple = self.df_missing.loc[row_num].tolist()
                    v = [self.val2idx[_v] for _v in full_tuple]
                    if self.flag_col:
                        # Adding column vector to fill in for missing value.
                        v[col_num] = self.col2idx[col]
                    else:
                        # Adding "dummy" vector position for later reindexing
                        v[col_num] = self.flag_index
                    if self.flag_rid:
                        # Adding rid to the beginning of the vector.
                        v = [row_num] + v
                    # if self.fd_dict is not None:
                    #     v = self.get_fd_context(v, col_num)
                    triplet = (row_num, col_id, true_value_id)
                    test_samples.append(v)
                    test_triplets.append(triplet)
                    positive_triplets += 1
                    positive_samples += 1
        boundaries.append(positive_triplets)

        print(f'{len(self.impossible)} values cannot be imputed.')
        triplets_tensor = torch.tensor(test_triplets, dtype=float)
        samples_tensor = torch.tensor(test_samples, dtype=int)
        samples_size = len(samples_tensor)
        return triplets_tensor, samples_tensor, samples_size, boundaries



    def get_positive_negative_samples_for_testing(self):
        positive_triplet_index = 0
        self.pos_edges_to_remove = torch.zeros(self.num_missing_values*2,2).to(int)
        # "Impossible" imputations are values that are only present in df_orig and do not appear in df_missing.
        self.impossible = []

        for row in range(self.num_rows):
            for col in range(self.num_columns):
                if self.df_missing.columns[col] in self.target_columns and \
                        pd.isnull(self.df_missing.iloc[row, col]) == True:
                    col_node_id = self.num_rows + col
                    tgt_val = self.df_orig.iloc[row, col]
                    #NOTE: we change again from df_missing to df_orig to get the correct value
                    try:
                        cell_node_id = self.val2idx[tgt_val]
                        self.test_positive_samples.append((row, col_node_id, cell_node_id))
                        positive_triplet_index += 1
                    except KeyError:
                        self.impossible.append(tgt_val)
                        # print(f'{tgt_val} cannot be imputed. ')
        print(f'{len(self.impossible)} imputation values cannot be imputed.')

        self.test_positive_samples = torch.tensor(self.test_positive_samples)
        print("positive test pos_triplets", len(self.test_positive_samples))
        self.test_positive_samples_size = len(self.test_positive_samples)

        self.test_positive_tuples = torch.zeros((self.test_positive_samples_size, self.input_tuple_length), dtype=int)
        self.test_positive_triplets = torch.IntTensor(self.test_positive_samples_size, 3)
        # self.test_negative_triplets = torch.IntTensor(self.test_negative_samples_size, 3)

        positive_samples = 0

        for sample in self.test_positive_samples:
            row_num, col_id, val_id = map(torch.Tensor.item, sample)
            self.test_positive_tuples[positive_samples, 0] = row_num
            full_tuple = self.df_missing.iloc[row_num].tolist()
            # full_tuple.remove(self.idx2val[val_id])
            col = self.idx2col[col_id]
            col_num = self.df_orig.columns.to_list().index(col)
            full_tuple[col_num] = '__flag_index__'
            try:
                self.test_positive_tuples[positive_samples, :] = torch.IntTensor([self.val2idx[v] if v in self.val2idx else self.flag_index for v in full_tuple ])
                self.test_positive_triplets[positive_samples, :] = torch.IntTensor((row_num, col_id, val_id))
            except KeyError:
                print('Found a problem')
            positive_samples += 1

        self.test_positive_tuples = self.test_positive_tuples.to(int)

    def get_statistics(self):
        statistics = {
            'num_rows': self.num_rows,
            'num_columns': self.num_columns,
            'num_imputation_columns': len(self.training_columns),
            'training_columns': '_'.join(self.training_columns),
            'training_rows': self.train_positive_samples_size,
            'num_missing_values': self.num_missing_values,
            'num_distinct_values': len(self.val2idx),
            'num_fds': self.num_fds,
            'fd_strategy': str(self.fd_strategy),
            'comb_num': self.num_combs,
            'comb_size': self.max_comb_size
        }
        if self.ext_features is not None:
            # TODO handle other types of node features
            statistics['node_features'] = 'external'
        else:
            if self.random_init:
                statistics['node_features'] = ['random_init']
            else:
                statistics['node_features'] = 'fixed_init'
        return statistics

    def process(self):
        print("Loading and computing basic stats")
        self.load_and_compute_stats()
        print("Creating graph structure")
        self.create_graph()
        if self.ext_features is not None:
            print('Loading external features')
            self.load_external_features()
        else:
            print("Computing graph features")
            self.compute_node_features()
        print("Creating positive and negative samples for training")
        self.create_train_positive_graph()
        print("Creating positive and negative samples for validation")
        self.create_valid_positive_graph()
        print("Creating positive and negative samples for testing")
        self.create_test_positive_negative_graphs()
        # if False:
        if self.architecture == 'multitask':
            self.labels, self.weights = self.generate_labels_multitask(self.train_positive_triplets)
            self.labels_valid, self.weights_valid = self.generate_labels_multitask(self.valid_positive_triplets)
        elif self.architecture == 'multilabel':
            self.labels, self.weights = self.generate_labels(self.train_positive_triplets)
            self.labels_valid, self.weights_valid = self.generate_labels(self.valid_positive_triplets)

