GRIMP
===

GRIMP is a data imputation system that imputes missing values in dirty datasets by relying
on Graph Neural Networks and attention.

`main_corruption.py` is used to generate dataset that contain missing values according to certain rules.

`main.py` is used to run the actual code.

# Datasets
All datasets and their respeective pre-trained embeddings will be uploaded on zenodo.

# Installation
We strongly recommend to run the code in a conda environment.

### Additional packages
The required packages are listed in `environment.yaml`.

Create a new conda environment:
```
conda env create -f environment.yaml
```
Then activate the new environment:
```
conda activate grimp
```


### Installing PyTorch
Install PyTorch following the instructions relative to your platform, as explained on [the website](https://pytorch.org/get-started/locally/).

If you have access to GPUs, GRIMP can make use of them to improve the execution time. Refer to the official documentation to install the GPU version of PyTorch.

# Running the code
To run the code, run `main.py` followed by the configuration file that contains the hyperparameters:
```sh
python main.py PATH_TO_CONFIG
```
The default configuration employed to run all experiments is provided in `config/default-config.toml`.

### Results
All results are saved in the folder `results`.
- `results/plots` contains the plot of the loss function for training and validation.
- `results/imputed_datasets` contains the imputed dataset produced by GRIMP at the end of the run.
- `results/json` contains the summary of the results in json format.
- `results/results/csv` is a log file that stores the result of each run in comma separated format.


# Preparing pretrained embeddings
The pretrained embeddings can be generated by using the `prepare_pretrained_embeddings.py` script
in the main folder.

This script is expecting the fasttext commoncrawl pretrained embedding corpus,
available at `https://fasttext.cc/docs/en/english-vectors.html`.
It will then list the files in the directory `data/to_pretrain` and generate embeddings for each of them, saving them
in the directory `data/pretrained-emb`. The dataset in `to_pretrain` should be the exact same datasets that will be used
in the training procedure as "dirty datasets".
