{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3310d6c-bdfa-45db-a131-57fddf59d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2df105d0-cf9c-4a12-9192-40b6c17d6169",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, q, k, v, device='cpu'):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.q = torch.tensor(q, requires_grad=True, device=device)\n",
    "        self.k = torch.tensor(k, requires_grad=True, device=device)\n",
    "        self.m = torch.ones_like(k)\n",
    "        self.m[self.k==1] = 0\n",
    "        self.v = torch.tensor(v, requires_grad=True, device=device)\n",
    "    def forward(self):\n",
    "        pass\n",
    "        print('in')\n",
    "        kq = torch.bmm(torch.transpose(self.k, 1,2), self.q)\n",
    "        sm = F.softmax(kq, dim=1)\n",
    "        mkq = torch.bmm(self.m, sm)\n",
    "        v_norm = F.normalize(self.v)\n",
    "        res = torch.bmm(mkq, v_norm)\n",
    "        return res.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_rows = 20\n",
    "num_cols = 3\n",
    "num_tok = 4\n",
    "n_nodes = num_rows + num_cols + num_tok\n",
    "dim_emb = 12\n",
    "batch_size = 4\n",
    "head_number = num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k = torch.ones((num_cols, num_cols))\n",
    "q = torch.zeros((num_cols, num_cols)).normal_()\n",
    "v = torch.empty(batch_size, num_cols,  dim_emb).normal_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "v_norm = F.normalize(v)\n",
    "for head in range(1):\n",
    "    weights = torch.nn.Softmax(dim=0)(torch.matmul(k[head], q))\n",
    "    m = torch.ones(num_cols)\n",
    "    m[head] = 0\n",
    "    weights = torch.mul(weights, m)\n",
    "    context_vector = torch.matmul(weights, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0137, 0.6277, 0.3586])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.nn.Softmax(dim=0)(torch.matmul(k[head], q))\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.6277, 0.3586])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mul(m, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0020, -0.2344, -0.1492, -0.8972,  0.6227,  1.4303, -0.1199, -0.8199,\n",
       "          0.0911,  0.5948,  0.7349, -0.8255],\n",
       "        [ 0.0902,  0.5959, -0.4133,  1.2431,  0.4643, -0.3567,  0.2688, -0.1477,\n",
       "          0.0307, -0.2530, -0.4103, -0.7421],\n",
       "        [ 0.6542,  1.3031, -0.0615, -0.0502, -0.5023,  0.3153,  1.2706, -0.4926,\n",
       "          0.7535,  0.0381,  0.2997, -0.4056],\n",
       "        [ 0.6793, -0.0234, -0.3701, -0.0131,  0.0238,  0.9076, -0.0194, -0.6491,\n",
       "          0.2089, -0.2606, -0.2858,  1.0482]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a52ef4cfbfd8a49c777add06af4fd65e7ed1d7e9699ed30e2f5899af5cc811db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
